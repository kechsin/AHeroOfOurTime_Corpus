{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2\n",
        "!pip install stanza\n",
        "!pip install detokenize"
      ],
      "metadata": {
        "id": "N2Miq2PN4Hai",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pymorphy2\n",
        "import stanza\n",
        "from detokenize.detokenizer import detokenize"
      ],
      "metadata": {
        "id": "BYNoL13Nozjb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "POS_TAGS = [\n",
        "    \"adj\",\n",
        "    \"adv\",\n",
        "    \"intj\",\n",
        "    \"noun\",\n",
        "    \"propn\",\n",
        "    \"verb\",\n",
        "    \"adp\",\n",
        "    \"aux\",\n",
        "    \"cconj\",\n",
        "    \"det\",\n",
        "    \"num\",\n",
        "    \"part\",\n",
        "    \"pron\",\n",
        "    \"sconj\",\n",
        "    \"x\"\n",
        "]"
      ],
      "metadata": {
        "id": "1K2rID4OA5IQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "\n",
        "class SearchEngineCSV:\n",
        "    def __init__(self, csv_file):\n",
        "        self.sentences = []\n",
        "        self.tokens = []\n",
        "        self.lemmas = []\n",
        "        self.pos_tags = []\n",
        "        self.chapters = []\n",
        "\n",
        "        with open(csv_file, newline='', encoding='utf-8') as file:\n",
        "            reader = csv.reader(file)\n",
        "            next(reader)\n",
        "            for row in reader:\n",
        "                self.sentences.append(row[0])\n",
        "                self.tokens.append([token.lower() for token in row[1].split(';')])  # Токены в нижнем регистре\n",
        "                self.lemmas.append([lemma.lower() for lemma in row[2].split(';')])  # Леммы в нижнем регистре\n",
        "                self.pos_tags.append(row[3].split(';'))\n",
        "                self.chapters.append(row[4])\n",
        "\n",
        "    def match_token(self, token, word, lemma, pos):\n",
        "        token = token.lower()  # Приведение токена к нижнему регистру\n",
        "        if '\"' in token:\n",
        "            return token.strip('\"') == word\n",
        "\n",
        "        if '+' in token:\n",
        "            query_word, query_pos = token.split('+')\n",
        "            return lemma.lower() == query_word.lower() and pos.lower() == query_pos.lower()\n",
        "\n",
        "        if token in POS_TAGS:\n",
        "            return pos.lower() == token\n",
        "\n",
        "        return lemma == morph.parse(token)[0].normal_form\n",
        "\n",
        "    def match_sequence(self, query_tokens, words, lemmas, pos_tags):\n",
        "        if len(query_tokens)>2 and '\"' in query_tokens[2]:\n",
        "            query_tokens[1] = query_tokens[1] + '\"'\n",
        "        for j, query_token in enumerate(query_tokens):\n",
        "            query_token = query_token.lower()  # Приведение токена запроса к нижнему регистру\n",
        "            if query_token in POS_TAGS:\n",
        "                if pos_tags[j].lower() != query_token:\n",
        "                    return False\n",
        "            else:\n",
        "                if not self.match_token(query_token, words[j], lemmas[j], pos_tags[j]):\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    def search(self, query):\n",
        "        query_tokens = query.lower().split()  # Приведение запроса к нижнему регистру\n",
        "\n",
        "        matches = []\n",
        "        for idx, sentence in enumerate(self.sentences):\n",
        "            words = self.tokens[idx]\n",
        "            lemmas = self.lemmas[idx]\n",
        "            pos_tags = self.pos_tags[idx]\n",
        "            if len(words) != len(lemmas) or len(words) != len(pos_tags):  # проверка, что количество токенов, лемм и тегов частей речи одинаковое\n",
        "                continue\n",
        "\n",
        "            # Поиск последовательности токенов в предложении\n",
        "            for i in range(len(words) - len(query_tokens) + 1):\n",
        "                formatted_sentence = \"\"\n",
        "                matches_in_sent = []\n",
        "                # Проверка последовательности для n-граммы\n",
        "                if self.match_sequence(query_tokens, words[i:i+len(query_tokens)], lemmas[i:i+len(query_tokens)], pos_tags[i:i+len(query_tokens)]):\n",
        "                    # Форматируем предложение с источником\n",
        "                    source_info = f\"[{self.chapters[idx]}]\"\n",
        "                    if formatted_sentence == \"\":\n",
        "                        formatted_sentence = f\"{sentence} {source_info}\"\n",
        "                    matches_in_sent.append((i, i + len(query_tokens)))  # информация о том, какие слова - ответ на запрос (номер первого слова последовательности и первого слова после неё)\n",
        "                if formatted_sentence != \"\":\n",
        "                    matches.append([formatted_sentence, matches_in_sent, idx])  # сначала строка \"текст предложения [номер главы]\", потом список мест, в которых слова, отвечающие на запрос\n",
        "\n",
        "        return matches\n"
      ],
      "metadata": {
        "id": "lfc760EnLznt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frequency(search_results, csv_file):  # функция, определяющая частотность, принимает результаты поиска\n",
        "    tokens = []  # список списков токенов в каждом предложении\n",
        "    with open(csv_file, newline='', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            tokens.append([token.lower() for token in row[1].split(';')])  # токены в нижнем регистре\n",
        "    freq_list = {}  # вариант ответа на запрос : его частотность\n",
        "    for results_elem in search_results:\n",
        "        sent = results_elem[0]\n",
        "        entry_indexes = results_elem[1]  # номера, на каких номерах слова, соответствующие самому запросу (номер первого слова и следующего после последнего слова)\n",
        "        sent_num = results_elem[2]\n",
        "        for i in entry_indexes:\n",
        "            entry_tokens = tokens[sent_num][i[0]:i[1]]\n",
        "            entry = detokenize(entry_tokens).replace('- ', '-')\n",
        "            freq_list[entry] = freq_list.get(entry, 0) + 1\n",
        "    freq_list_array = list(freq_list.items())\n",
        "    freq_list_sorted =  sorted(freq_list_array, key=lambda x: x[1], reverse=True)\n",
        "    return freq_list_sorted"
      ],
      "metadata": {
        "id": "Fkb5DhGlfX4X"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = '/content/corpus_data_with_labels.csv'\n",
        "search_engine = SearchEngineCSV(csv_file)"
      ],
      "metadata": {
        "id": "UzWt540l_ez1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequency(search_engine.search('NOUN'), csv_file)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7ffCI_DdIOj",
        "outputId": "555b0605-5634-4062-e32f-ed691077f0ec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('глаза', 49),\n",
              " ('раз', 45),\n",
              " ('руку', 33),\n",
              " ('княжна', 29),\n",
              " ('человек', 27),\n",
              " ('день', 26),\n",
              " ('разговор', 25),\n",
              " ('сердце', 24),\n",
              " ('слова', 22),\n",
              " ('друг', 21)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(frequency(search_engine.search(input('Запрос для подсчёта частотностей: ')), csv_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY11kGvcGGY8",
        "outputId": "165b16b9-2cb5-4f72-c27b-416cb2c865a9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Запрос для подсчёта частотностей: конь\n",
            "[('коня', 10), ('конь', 4), ('конях', 1), ('коне', 1), ('кони', 1)]\n"
          ]
        }
      ]
    }
  ]
}